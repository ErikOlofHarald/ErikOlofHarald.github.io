<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Fasttext on Hunches &amp; Crunches</title>
    <link>/tags/fasttext/</link>
    <description>Recent content in Fasttext on Hunches &amp; Crunches</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Copyright © 2008–2018, Erik Andreasson</copyright>
    <lastBuildDate>Tue, 01 Sep 2020 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/tags/fasttext/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>fastText: Words, subwords &amp; n-grams</title>
      <link>/2020/09/01/fasttext/</link>
      <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/01/fasttext/</guid>
      <description>FastText is an algorithm developed by Facebook that can be used for both word representation and text classification. This post goes through some of the inner workings of the text classifier, in particular how predictions are obtained for a trained model with both n-grams and subword features. The purpose is not to find an optimal model but to bring some clarity to how the algorithm works behind the scenes.</description>
    </item>
    
  </channel>
</rss>
